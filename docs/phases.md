# RealChat Implementation Phases

**Implementation-Grade Execution Sequencing**

This is not a demo. This is distributed infrastructure that must survive crashes, scale, and evolve.

Each phase is structured as:
1. Phase intent
2. Internal architecture evolution
3. Data model
4. Failure modes
5. Operational maturity
6. Exit criteria

No hand-waving.

---

## ðŸ§± Phase 0 â€” Architecture & Contract Freeze

### ðŸŽ¯ Goal

Lock system invariants before code.

If invariants are wrong, everything downstream is garbage.

---

### 0.1 Define Non-Negotiables

You must explicitly define:

* **Message ordering guarantee**
  * Per conversation? (Recommended)
  * Per user?
* **Delivery semantics**
  * At-least-once (practical)
  * Exactly-once (illusion via dedup)
* **Latency SLA**
  * P99 < 150ms intra-region?
* **Availability target**
  * 99.9%?
* **Max room size**
  * Impacts fanout model

---

### 0.2 Message Envelope Contract

Immutable schema. Versioned.

```json
{
  "message_id": "UUID",
  "conversation_id": "UUID",
  "sender_id": "UUID",
  "sequence_number": "int64",
  "payload": "bytes",
  "created_at": "timestamp",
  "idempotency_key": "string"
}
```

**Why idempotency key?**  
Because retries are inevitable.

---

### 0.3 Partitioning Strategy (Critical)

If using Kafka:

**Partition key = `conversation_id`**

**Why?**  
Because ordering is preserved per partition.

**Tradeoff:**  
Hot conversations create hot partitions.

**Mitigation later via sharding.**

---

### 0.4 Exit Criteria

* âœ“ API spec frozen
* âœ“ Data model finalized
* âœ“ Failure semantics documented
* âœ“ Ordering semantics documented
* âœ“ All edge cases enumerated

**If not, do not code.**

---

## ðŸ§± Phase 1 â€” Single Node, Correctness First

### ðŸŽ¯ Goal

Deliver a correct end-to-end message pipeline on ONE machine.

---

### 1.1 Architecture

```
Client â†’ WS Server â†’ Message Handler â†’ Postgres â†’ In-memory fanout â†’ Client
```

**No Kafka. No Redis.**

---

### 1.2 Core Components

#### WebSocket Manager

**Responsibilities:**
* Connection registry
* Heartbeat
* Backpressure detection
* Graceful disconnect

**Data structure:**
```go
map[userID] -> connection
```

**Must handle:**
* Concurrent writes
* Race during disconnect

---

#### Message Service

**Responsibilities:**
* Validate sender
* Validate conversation membership
* Generate sequence number
* Persist message
* Trigger fanout

**Sequence generation:**  
Use DB atomic increment per conversation.

**Do NOT compute in memory.**

---

### 1.3 Data Model

**Messages table:**

```sql
CREATE TABLE messages (
  id UUID PRIMARY KEY,
  conversation_id UUID NOT NULL,
  sender_id UUID NOT NULL,
  sequence BIGINT NOT NULL,
  payload JSONB NOT NULL,
  created_at TIMESTAMP NOT NULL DEFAULT NOW(),
  
  UNIQUE(conversation_id, sequence)
);

CREATE INDEX idx_messages_conversation ON messages(conversation_id, sequence);
```

**This guarantees ordering correctness.**

---

### 1.4 Failure Handling

**Case:** DB write succeeds but WS delivery fails.

**System behavior:**
* Message remains persisted.
* Client will fetch on reconnect.

**That's eventual delivery.**

---

### 1.5 Complexity

**Time:**
* Write: O(1)
* Fanout: O(n) where n = participants

**Space:**
* O(active_connections)

---

### 1.6 Exit Criteria

* âœ“ No duplicate messages under retry
* âœ“ Correct ordering under concurrent send
* âœ“ Stable under 10k connections on single node
* âœ“ Memory footprint predictable

**Only then move forward.**

---

## ðŸ§± Phase 2 â€” Introduce Event-Driven Decoupling

Now we introduce Kafka.

---

### ðŸŽ¯ Goal

Separate ingestion from delivery.

---

### 2.1 New Flow

```
Client â†’ WS â†’ Produce to Kafka
Kafka â†’ Message Consumer â†’ DB
Kafka â†’ Delivery Consumer â†’ WS nodes
```

---

### 2.2 Why This Matters

Now:
* Writes are asynchronous
* You can replay
* You decouple compute domains

---

### 2.3 Idempotency

**Producer:**
* Enable idempotent producer
* Attach idempotency_key

**Consumer:**
* Maintain dedup table:

```sql
CREATE TABLE processed_messages (
  idempotency_key VARCHAR(255) PRIMARY KEY,
  processed_at TIMESTAMP NOT NULL DEFAULT NOW()
);
```

**Before writing:**  
Check existence.

---

### 2.4 Retry Strategy

Exponential backoff.

**If max retries exceeded:**  
Send to DLQ.

**Never silently drop.**

---

### 2.5 Ordering Guarantee

**Because partition key = conversation_id:**  
Ordering preserved per conversation.

**If you choose random partitioning:**  
You destroy ordering guarantee.

---

### 2.6 Failure Modes

#### Kafka down

Gateway must:
* Apply backpressure
* Reject new messages (HTTP 503)
* Or buffer in memory (risky)

**Never infinite buffer.**

---

### 2.7 Exit Criteria

* âœ“ Replay from offset works
* âœ“ Kill consumer â†’ restart â†’ no data loss
* âœ“ Inject duplicate messages â†’ dedup works
* âœ“ Consumer lag observable

---

## ðŸ§± Phase 3 â€” Horizontal WebSocket Layer

Now complexity increases sharply.

---

### ðŸŽ¯ Goal

Scale to multiple WS nodes.

---

### 3.1 Problem

* User A connected to node 1
* User B connected to node 2

**How does delivery happen?**

---

### 3.2 Solution Pattern

Use Redis:

```
user:{user_id} -> ws_node_id
node:{node_id} -> connection_list
```

**Delivery flow:**
1. Delivery consumer determines recipient
2. Lookup Redis
3. Publish to correct node

---

### 3.3 Pub/Sub vs Direct RPC

**Option 1:**  
Redis PubSub (simple, less reliable)

**Option 2:**  
Dedicated delivery topic per node (recommended)

---

### 3.4 Backpressure Handling

If WS node overloaded:
* Stop consuming from delivery topic
* Consumer lag increases
* Autoscaler triggers

---

### 3.5 Load Balancing

**Do NOT rely on sticky sessions alone.**

WebSocket is long-lived.  
Load imbalance happens over time.

**Implement:**
* Connection draining on deployment
* Graceful shutdown with timeout

---

### 3.6 Exit Criteria

* âœ“ 100k+ concurrent connections
* âœ“ Horizontal scaling works
* âœ“ Killing 1 node does not drop messages permanently
* âœ“ Redis failover tested

---

## ðŸ§± Phase 4 â€” Reliability Engineering

Now we make it production-worthy.

---

### 4.1 Timeouts Everywhere

Every network call:
* DB
* Redis
* Kafka

**Must have timeout.**

**Otherwise:**  
Thread exhaustion.

---

### 4.2 Circuit Breakers

If Redis latency spikes:
* Trip breaker.
* Fail fast.

---

### 4.3 Graceful Shutdown

When pod terminating:
1. Stop accepting new connections
2. Drain existing
3. Commit offsets
4. Close Kafka consumer cleanly

---

### 4.4 Observability

#### Metrics
* Active connections
* Kafka lag
* DB write latency
* P99 end-to-end latency
* Failed deliveries

#### Tracing
* Trace ID per message

#### Logs
* Structured JSON logs only

---

### 4.5 Chaos Testing

**Kill:**
* Kafka broker
* Redis node
* Random WS node

**System must degrade gracefully.**

---

## ðŸ§± Phase 5 â€” Product Complexity

Now comes the real scaling stress.

---

### 5.1 Large Groups (Fanout Explosion)

If group size = 10k:

**Naive O(n) fanout per message is expensive.**

**Optimizations:**
* Batch delivery
* Lazy pull model
* Notification pointer model

---

### 5.2 Offline Users

On reconnect:
1. Query DB by sequence
2. Deliver missing messages
3. Update last_seen_sequence

**This requires:**

```sql
CREATE TABLE user_conversation_state (
  user_id UUID NOT NULL,
  conversation_id UUID NOT NULL,
  last_delivered_sequence BIGINT NOT NULL DEFAULT 0,
  
  PRIMARY KEY (user_id, conversation_id)
);
```

---

### 5.3 Read Receipts

**Model:**
* Store read sequence per user

**Never store per-message read rows.**  
Explodes storage.

---

## ðŸ§± Phase 6 â€” Performance Engineering

Now think like infra.

---

### 6.1 Load Testing

**Simulate:**
* 1M connections
* 10k messages/sec

**Measure:**
* P50, P95, P99
* GC pauses
* CPU saturation
* Kafka lag

---

### 6.2 Optimization Levers

* Batch DB writes
* Kafka batch producer
* Connection pooling
* Reduce JSON allocations
* Use binary protocol internally

---

### 6.3 Memory Math

```
If 1 connection = 50KB
1M connections = 50GB RAM
```

**Can your infra afford that?**

Always compute memory per connection.

---

## ðŸ§± Phase 7 â€” Multi-Region

Now things get real.

**Questions:**
* Global ordering?
* Active-active?
* Conflict resolution?

**Tradeoffs:**
* Strong consistency â†’ higher latency
* Eventual consistency â†’ simpler

**Most systems choose:**  
Regional isolation + eventual sync.

---

## ðŸ§  What Most Engineers Do Wrong

They:
* Add Kafka before correctness
* Add Redis before reasoning
* Add Kubernetes before stability

**Correct order:**  
Correctness â†’ Decoupling â†’ Scalability â†’ Reliability â†’ Optimization.

---

## ðŸ“‹ Phase Summary

| Phase | Focus | Key Technology | Exit Metric |
|-------|-------|----------------|-------------|
| 0 | Architecture | Design docs | Contract frozen |
| 1 | Correctness | Single node + Postgres | 10k connections stable |
| 2 | Decoupling | Kafka | Replay works |
| 3 | Scale | Redis + Multi-node | 100k connections |
| 4 | Reliability | Timeouts + Circuit breakers | Chaos tested |
| 5 | Product | Offline sync + Large groups | 10k user rooms |
| 6 | Performance | Load testing | P99 < 150ms |
| 7 | Global | Multi-region | Regional failover |

---

## ðŸš¨ Critical Principles

1. **Never skip phases** â€” Each builds on the previous
2. **Exit criteria are mandatory** â€” No fuzzy "good enough"
3. **Failure modes first** â€” Design for failure, not success
4. **Measure everything** â€” No observability = no production
5. **Simplicity wins** â€” Add complexity only when forced

---

**This is how you build systems that last.**
